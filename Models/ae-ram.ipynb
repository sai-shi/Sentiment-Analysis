{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "iterations = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "reg_eta = 0.001\n",
    "\n",
    "# dimensionalities\n",
    "dim_lstm = 64\n",
    "dim_gru = 64\n",
    "dim_word = 300\n",
    "dim_aspect = 5\n",
    "dim_aspect_embedding = 64\n",
    "dim_sentence = 80\n",
    "dim_polarity = 3\n",
    "\n",
    "# setup utils object\n",
    "u = utils.UTILS(batch_size, dim_sentence, dim_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tf placeholders\n",
    "X = tf.placeholder(tf.int32, [None, dim_sentence])\n",
    "y = tf.placeholder(tf.float32, [None, dim_polarity])\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "aspects = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# define tf variables\n",
    "with tf.variable_scope('aspect_embedding_vars', reuse = tf.AUTO_REUSE):\n",
    "    fw_va = tf.get_variable(\n",
    "        name = 'aspect_matrix_forward_Va',\n",
    "        shape = [dim_aspect, dim_aspect_embedding],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    bk_va = tf.get_variable(\n",
    "        name = 'aspect_matrix_backward_Va',\n",
    "        shape = [dim_aspect, dim_aspect_embedding],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wv = tf.get_variable(\n",
    "        name = 'aspect_Wv',\n",
    "        shape = [dim_aspect_embedding * 2, dim_aspect_embedding * 2],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "with tf.variable_scope('attention_vars', reuse = tf.AUTO_REUSE):\n",
    "    wm = tf.get_variable(\n",
    "        name = 'M_Wm',\n",
    "        shape = [dim_lstm * 2, dim_lstm * 2],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    bm = tf.get_variable(\n",
    "        name = 'M_Bm',\n",
    "        shape = [dim_lstm * 2],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    with tf.variable_scope('gru_vars', reuse = tf.AUTO_REUSE):\n",
    "        wr = tf.get_variable(\n",
    "            name = 'r_iAL_Wr',\n",
    "            shape = [dim_gru, dim_lstm * 2],\n",
    "            initializer = tf.random_normal_initializer(0, 0.003),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "        )\n",
    "        ur = tf.get_variable(\n",
    "            name = 'r_e_Ur',\n",
    "            shape = [dim_gru, dim_gru],\n",
    "            initializer = tf.random_normal_initializer(0, 0.003),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "        )\n",
    "        wz = tf.get_variable(\n",
    "            name = 'z_iAL_Wz',\n",
    "            shape = [dim_gru, dim_lstm * 2],\n",
    "            initializer = tf.random_normal_initializer(0, 0.003),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "        )\n",
    "        uz = tf.get_variable(\n",
    "            name = 'z_e_Uz',\n",
    "            shape = [dim_gru, dim_gru],\n",
    "            initializer = tf.random_normal_initializer(0, 0.003),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "        )\n",
    "        wx = tf.get_variable(\n",
    "            name = 'e_tanh_Wx',\n",
    "            shape = [dim_gru, dim_lstm * 2],\n",
    "            initializer = tf.random_normal_initializer(0, 0.003),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "        )\n",
    "        wg = tf.get_variable(\n",
    "            name = 'e_tanh_Wg',\n",
    "            shape = [dim_gru, dim_gru],\n",
    "            initializer = tf.random_normal_initializer(0, 0.003),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "        )\n",
    "with tf.variable_scope('output_softmax_vars', reuse = tf.AUTO_REUSE):\n",
    "    ws = tf.get_variable(\n",
    "        name = 'y_softmax_Wy',\n",
    "        shape = [dim_gru, dim_polarity],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    bs = tf.get_variable(\n",
    "        name = 'y_softmax_By',\n",
    "        shape = [dim_polarity],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bi lstm model\n",
    "def bi_lstm(inputs):\n",
    "#     inputs = tf.nn.dropout(inputs, keep_prob=1.0)\n",
    "    with tf.name_scope('lstm_model'):\n",
    "#         # slice the corresponding vai from va\n",
    "#         fw_vai = tf.gather(fw_va, aspects) # batch_size x dim_aspect_embedding\n",
    "#         bk_vai = tf.gather(bk_va, aspects) # batch_size x dim_aspect_embedding\n",
    "#         # concatenate vai to inputs\n",
    "#         vai_en = [vai for i in range(dim_sentence)]\n",
    "#         vai_en = tf.stack(vai_en, axis = 1) # batch_size x dim_sentence x dim_aspect_embedding\n",
    "#         inputs = tf.concat([inputs, vai_en], 2)\n",
    "        forward_lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm)\n",
    "        backward_lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm)\n",
    "        H, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_lstm_cell,\n",
    "            backward_lstm_cell,\n",
    "            inputs = inputs,\n",
    "            sequence_length = seqlen,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'bilstm'\n",
    "        )\n",
    "        M = tf.concat(H, 2) # batch_size x dim_sentence x (dim_lstm * 2)\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ram model\n",
    "def ram(M):\n",
    "    size = tf.shape(M)[0] # batch_size\n",
    "    # attention layer 1\n",
    "    e0 = tf.zeros([size, dim_gru]) # batch_size x dim_gru\n",
    "    g = tf.matmul(tf.reshape(M, [-1, dim_lstm * 2]), wm) + bm # (batch_size * dim_sentence) x (dim_lstm * 2)\n",
    "    alpha = tf.nn.softmax(g)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# define operations\n",
    "# tf.reset_default_graph()\n",
    "pred = dynamic_lstm(tf.nn.embedding_lookup(u.gloveDict, X), seqlen, aspects)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 1.0672083, train accuracy: 0.60947865\n",
      "step: 0, test loss: 1.0597196, test accuracy: 0.6438356\n",
      "step: 1, train loss: 1.0270233, train accuracy: 0.61042655\n",
      "step: 1, test loss: 1.0090522, test accuracy: 0.6438356\n",
      "step: 2, train loss: 0.98581773, train accuracy: 0.61042655\n",
      "step: 2, test loss: 0.9563357, test accuracy: 0.6438356\n",
      "step: 3, train loss: 0.9560119, train accuracy: 0.61042655\n",
      "step: 3, test loss: 0.9161578, test accuracy: 0.6438356\n",
      "step: 4, train loss: 0.94279426, train accuracy: 0.61042655\n",
      "step: 4, test loss: 0.8936038, test accuracy: 0.6438356\n",
      "step: 5, train loss: 0.94123256, train accuracy: 0.61042655\n",
      "step: 5, test loss: 0.8825179, test accuracy: 0.6438356\n",
      "step: 6, train loss: 0.94334894, train accuracy: 0.61042655\n",
      "step: 6, test loss: 0.8755773, test accuracy: 0.6438356\n",
      "step: 7, train loss: 0.94440395, train accuracy: 0.61042655\n",
      "step: 7, test loss: 0.8691451, test accuracy: 0.6438356\n",
      "step: 8, train loss: 0.94331676, train accuracy: 0.61042655\n",
      "step: 8, test loss: 0.86267406, test accuracy: 0.6438356\n",
      "step: 9, train loss: 0.94118243, train accuracy: 0.61042655\n",
      "step: 9, test loss: 0.8573678, test accuracy: 0.6438356\n",
      "step: 10, train loss: 0.9395407, train accuracy: 0.61042655\n",
      "step: 10, test loss: 0.8547431, test accuracy: 0.6438356\n",
      "step: 11, train loss: 0.9389464, train accuracy: 0.61042655\n",
      "step: 11, test loss: 0.8552127, test accuracy: 0.6438356\n",
      "step: 12, train loss: 0.93833643, train accuracy: 0.61042655\n",
      "step: 12, test loss: 0.8574917, test accuracy: 0.6438356\n",
      "step: 13, train loss: 0.9368741, train accuracy: 0.61042655\n",
      "step: 13, test loss: 0.86031926, test accuracy: 0.6438356\n",
      "step: 14, train loss: 0.934617, train accuracy: 0.61042655\n",
      "step: 14, test loss: 0.86319965, test accuracy: 0.6438356\n",
      "step: 15, train loss: 0.9321509, train accuracy: 0.61042655\n",
      "step: 15, test loss: 0.8662412, test accuracy: 0.6438356\n",
      "step: 16, train loss: 0.93018544, train accuracy: 0.61042655\n",
      "step: 16, test loss: 0.86966753, test accuracy: 0.6438356\n",
      "step: 17, train loss: 0.93012446, train accuracy: 0.61042655\n",
      "step: 17, test loss: 0.8743775, test accuracy: 0.6438356\n",
      "step: 18, train loss: 0.931032, train accuracy: 0.61042655\n",
      "step: 18, test loss: 0.87914556, test accuracy: 0.6438356\n",
      "step: 19, train loss: 0.93197834, train accuracy: 0.61042655\n",
      "step: 19, test loss: 0.88282377, test accuracy: 0.6438356\n",
      "step: 20, train loss: 0.932167, train accuracy: 0.61042655\n",
      "step: 20, test loss: 0.8845022, test accuracy: 0.6438356\n",
      "step: 21, train loss: 0.93152714, train accuracy: 0.61042655\n",
      "step: 21, test loss: 0.8841258, test accuracy: 0.6438356\n",
      "step: 22, train loss: 0.9308177, train accuracy: 0.61042655\n",
      "step: 22, test loss: 0.8826008, test accuracy: 0.6438356\n",
      "step: 23, train loss: 0.92924345, train accuracy: 0.61042655\n",
      "step: 23, test loss: 0.87943417, test accuracy: 0.6438356\n",
      "step: 24, train loss: 0.92765796, train accuracy: 0.61042655\n",
      "step: 24, test loss: 0.8755789, test accuracy: 0.6438356\n",
      "step: 25, train loss: 0.9256325, train accuracy: 0.61042655\n",
      "step: 25, test loss: 0.8704184, test accuracy: 0.6438356\n",
      "step: 26, train loss: 0.9227407, train accuracy: 0.61042655\n",
      "step: 26, test loss: 0.86382675, test accuracy: 0.6438356\n",
      "step: 27, train loss: 0.9215289, train accuracy: 0.61042655\n",
      "step: 27, test loss: 0.8595083, test accuracy: 0.6438356\n",
      "step: 28, train loss: 0.91903514, train accuracy: 0.61042655\n",
      "step: 28, test loss: 0.8542044, test accuracy: 0.6438356\n",
      "step: 29, train loss: 0.9160702, train accuracy: 0.61042655\n",
      "step: 29, test loss: 0.84900165, test accuracy: 0.6438356\n",
      "step: 30, train loss: 0.91422415, train accuracy: 0.61042655\n",
      "step: 30, test loss: 0.8456918, test accuracy: 0.6438356\n",
      "step: 31, train loss: 0.9110252, train accuracy: 0.61042655\n",
      "step: 31, test loss: 0.8421926, test accuracy: 0.6438356\n",
      "step: 32, train loss: 0.9069989, train accuracy: 0.61042655\n",
      "step: 32, test loss: 0.8388402, test accuracy: 0.6438356\n",
      "step: 33, train loss: 0.90249544, train accuracy: 0.61042655\n",
      "step: 33, test loss: 0.8353985, test accuracy: 0.6438356\n",
      "step: 34, train loss: 0.897712, train accuracy: 0.61042655\n",
      "step: 34, test loss: 0.8317161, test accuracy: 0.6438356\n",
      "step: 35, train loss: 0.8922939, train accuracy: 0.61042655\n",
      "step: 35, test loss: 0.8280161, test accuracy: 0.6438356\n",
      "step: 36, train loss: 0.88647586, train accuracy: 0.61042655\n",
      "step: 36, test loss: 0.82393456, test accuracy: 0.6438356\n",
      "step: 37, train loss: 0.8801927, train accuracy: 0.61042655\n",
      "step: 37, test loss: 0.8188928, test accuracy: 0.6438356\n",
      "step: 38, train loss: 0.87304306, train accuracy: 0.61042655\n",
      "step: 38, test loss: 0.8144359, test accuracy: 0.6438356\n",
      "step: 39, train loss: 0.8660412, train accuracy: 0.61042655\n",
      "step: 39, test loss: 0.80816853, test accuracy: 0.6438356\n",
      "step: 40, train loss: 0.8580859, train accuracy: 0.61042655\n",
      "step: 40, test loss: 0.8051245, test accuracy: 0.6438356\n",
      "step: 41, train loss: 0.8503285, train accuracy: 0.61042655\n",
      "step: 41, test loss: 0.8006584, test accuracy: 0.6438356\n",
      "step: 42, train loss: 0.84374785, train accuracy: 0.61042655\n",
      "step: 42, test loss: 0.79562354, test accuracy: 0.6438356\n",
      "step: 43, train loss: 0.83620965, train accuracy: 0.61042655\n",
      "step: 43, test loss: 0.7921556, test accuracy: 0.6438356\n",
      "step: 44, train loss: 0.8273073, train accuracy: 0.61042655\n",
      "step: 44, test loss: 0.7851795, test accuracy: 0.6438356\n",
      "step: 45, train loss: 0.8186725, train accuracy: 0.61042655\n",
      "step: 45, test loss: 0.77892673, test accuracy: 0.6438356\n",
      "step: 46, train loss: 0.8112013, train accuracy: 0.61042655\n",
      "step: 46, test loss: 0.7726446, test accuracy: 0.6438356\n",
      "step: 47, train loss: 0.80096346, train accuracy: 0.61042655\n",
      "step: 47, test loss: 0.76517254, test accuracy: 0.6438356\n",
      "step: 48, train loss: 0.7920046, train accuracy: 0.61042655\n",
      "step: 48, test loss: 0.75830144, test accuracy: 0.6438356\n",
      "step: 49, train loss: 0.78879887, train accuracy: 0.6113744\n",
      "step: 49, test loss: 0.75678265, test accuracy: 0.6438356\n",
      "step: 50, train loss: 0.7739522, train accuracy: 0.6113744\n",
      "step: 50, test loss: 0.7466875, test accuracy: 0.6438356\n",
      "step: 51, train loss: 0.7675517, train accuracy: 0.6123223\n",
      "step: 51, test loss: 0.7431171, test accuracy: 0.6438356\n",
      "step: 52, train loss: 0.7605289, train accuracy: 0.6123223\n",
      "step: 52, test loss: 0.73851764, test accuracy: 0.6438356\n",
      "step: 53, train loss: 0.7525505, train accuracy: 0.6123223\n",
      "step: 53, test loss: 0.7357519, test accuracy: 0.6438356\n",
      "step: 54, train loss: 0.73931485, train accuracy: 0.6123223\n",
      "step: 54, test loss: 0.72761387, test accuracy: 0.6438356\n",
      "step: 55, train loss: 0.7343758, train accuracy: 0.6549763\n",
      "step: 55, test loss: 0.726904, test accuracy: 0.68835616\n",
      "step: 56, train loss: 0.72103, train accuracy: 0.6947867\n",
      "step: 56, test loss: 0.72085154, test accuracy: 0.7328767\n",
      "step: 57, train loss: 0.7176245, train accuracy: 0.7099526\n",
      "step: 57, test loss: 0.7206143, test accuracy: 0.7328767\n",
      "step: 58, train loss: 0.7032914, train accuracy: 0.7317535\n",
      "step: 58, test loss: 0.71416306, test accuracy: 0.75\n",
      "step: 59, train loss: 0.69460475, train accuracy: 0.74407583\n",
      "step: 59, test loss: 0.71199626, test accuracy: 0.75342464\n",
      "step: 60, train loss: 0.68902415, train accuracy: 0.7336493\n",
      "step: 60, test loss: 0.70544726, test accuracy: 0.75342464\n",
      "step: 61, train loss: 0.67399186, train accuracy: 0.7507109\n",
      "step: 61, test loss: 0.70534736, test accuracy: 0.76369864\n",
      "step: 62, train loss: 0.66577923, train accuracy: 0.749763\n",
      "step: 62, test loss: 0.70589745, test accuracy: 0.7568493\n",
      "step: 63, train loss: 0.6597021, train accuracy: 0.7488152\n",
      "step: 63, test loss: 0.69716895, test accuracy: 0.7671233\n",
      "step: 64, train loss: 0.65733653, train accuracy: 0.7516588\n",
      "step: 64, test loss: 0.6955102, test accuracy: 0.75342464\n",
      "step: 65, train loss: 0.63867813, train accuracy: 0.7563981\n",
      "step: 65, test loss: 0.6935338, test accuracy: 0.76369864\n",
      "step: 66, train loss: 0.63186663, train accuracy: 0.7563981\n",
      "step: 66, test loss: 0.6970488, test accuracy: 0.75342464\n",
      "step: 67, train loss: 0.62574583, train accuracy: 0.7563981\n",
      "step: 67, test loss: 0.6936066, test accuracy: 0.760274\n",
      "step: 68, train loss: 0.616108, train accuracy: 0.76398104\n",
      "step: 68, test loss: 0.69837064, test accuracy: 0.75\n",
      "step: 69, train loss: 0.6062463, train accuracy: 0.76682466\n",
      "step: 69, test loss: 0.69708043, test accuracy: 0.75342464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 70, train loss: 0.59780127, train accuracy: 0.7687204\n",
      "step: 70, test loss: 0.69510806, test accuracy: 0.760274\n",
      "step: 71, train loss: 0.59004873, train accuracy: 0.771564\n",
      "step: 71, test loss: 0.7010236, test accuracy: 0.7568493\n",
      "step: 72, train loss: 0.58453083, train accuracy: 0.7696682\n",
      "step: 72, test loss: 0.7065754, test accuracy: 0.760274\n",
      "step: 73, train loss: 0.5784995, train accuracy: 0.77251184\n",
      "step: 73, test loss: 0.7097099, test accuracy: 0.7568493\n",
      "step: 74, train loss: 0.5732919, train accuracy: 0.77251184\n",
      "step: 74, test loss: 0.7142982, test accuracy: 0.75342464\n",
      "step: 75, train loss: 0.57293886, train accuracy: 0.7706161\n",
      "step: 75, test loss: 0.73340124, test accuracy: 0.739726\n",
      "step: 76, train loss: 0.58376986, train accuracy: 0.76303315\n",
      "step: 76, test loss: 0.7111329, test accuracy: 0.7671233\n",
      "step: 77, train loss: 0.61227053, train accuracy: 0.7402844\n",
      "step: 77, test loss: 0.73336107, test accuracy: 0.71575344\n",
      "step: 78, train loss: 0.5642774, train accuracy: 0.7706161\n",
      "step: 78, test loss: 0.7326663, test accuracy: 0.739726\n",
      "step: 79, train loss: 0.60499173, train accuracy: 0.75545025\n",
      "step: 79, test loss: 0.7382778, test accuracy: 0.760274\n",
      "step: 80, train loss: 0.5650254, train accuracy: 0.7696682\n",
      "step: 80, test loss: 0.7321701, test accuracy: 0.75342464\n",
      "step: 81, train loss: 0.55965954, train accuracy: 0.771564\n",
      "step: 81, test loss: 0.7188144, test accuracy: 0.73630136\n",
      "step: 82, train loss: 0.5807359, train accuracy: 0.7563981\n",
      "step: 82, test loss: 0.724287, test accuracy: 0.7294521\n",
      "step: 83, train loss: 0.5525407, train accuracy: 0.77440757\n",
      "step: 83, test loss: 0.70579714, test accuracy: 0.7568493\n",
      "step: 84, train loss: 0.5647401, train accuracy: 0.7706161\n",
      "step: 84, test loss: 0.73330134, test accuracy: 0.7568493\n",
      "step: 85, train loss: 0.5524676, train accuracy: 0.7763033\n",
      "step: 85, test loss: 0.7175503, test accuracy: 0.760274\n",
      "step: 86, train loss: 0.54131764, train accuracy: 0.7781991\n",
      "step: 86, test loss: 0.7082415, test accuracy: 0.76369864\n",
      "step: 87, train loss: 0.54265875, train accuracy: 0.77535546\n",
      "step: 87, test loss: 0.7148381, test accuracy: 0.760274\n",
      "step: 88, train loss: 0.5463365, train accuracy: 0.7696682\n",
      "step: 88, test loss: 0.7259811, test accuracy: 0.75342464\n",
      "step: 89, train loss: 0.53160256, train accuracy: 0.7781991\n",
      "step: 89, test loss: 0.71146345, test accuracy: 0.7568493\n",
      "step: 90, train loss: 0.5336297, train accuracy: 0.7781991\n",
      "step: 90, test loss: 0.7248708, test accuracy: 0.760274\n",
      "step: 91, train loss: 0.5336058, train accuracy: 0.7800948\n",
      "step: 91, test loss: 0.7319259, test accuracy: 0.76369864\n",
      "step: 92, train loss: 0.52084005, train accuracy: 0.7819905\n",
      "step: 92, test loss: 0.7189902, test accuracy: 0.7568493\n",
      "step: 93, train loss: 0.5147745, train accuracy: 0.78483415\n",
      "step: 93, test loss: 0.70268446, test accuracy: 0.75342464\n",
      "step: 94, train loss: 0.5134149, train accuracy: 0.7876777\n",
      "step: 94, test loss: 0.70502555, test accuracy: 0.7568493\n",
      "step: 95, train loss: 0.5066567, train accuracy: 0.79241705\n",
      "step: 95, test loss: 0.7064391, test accuracy: 0.75342464\n",
      "step: 96, train loss: 0.49867463, train accuracy: 0.8\n",
      "step: 96, test loss: 0.7263209, test accuracy: 0.75342464\n",
      "step: 97, train loss: 0.5016138, train accuracy: 0.7981043\n",
      "step: 97, test loss: 0.7515354, test accuracy: 0.760274\n",
      "step: 98, train loss: 0.49093977, train accuracy: 0.80189574\n",
      "step: 98, test loss: 0.74709237, test accuracy: 0.75342464\n",
      "step: 99, train loss: 0.4856554, train accuracy: 0.8066351\n",
      "step: 99, test loss: 0.72146523, test accuracy: 0.7431507\n"
     ]
    }
   ],
   "source": [
    "# full dataset training\n",
    "test_X, test_y, test_seqlen, test_aspects = u.getData('test')\n",
    "train_X, train_y, train_seqlen, train_aspects = u.getData('train')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        sess.run(optimizer, feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "#         if i > 0 and i % 4 == 0:\n",
    "        loss_train, accuracy_train = sess.run([loss, accuracy], feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "        print('step: %s, train loss: %s, train accuracy: %s' % (i, loss_train, accuracy_train))\n",
    "        loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, aspects: test_aspects})\n",
    "        print('step: %s, test loss: %s, test accuracy: %s' % (i, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4, train loss: 1.0650923, train accuracy: 0.34375\n",
      "step: 4, test loss: 1.0683516, test accuracy: 0.43493152\n",
      "step: 8, train loss: 1.0652124, train accuracy: 0.3125\n",
      "step: 8, test loss: 1.0012907, test accuracy: 0.65753424\n",
      "step: 12, train loss: 0.99717486, train accuracy: 0.65625\n",
      "step: 12, test loss: 0.9812017, test accuracy: 0.5753425\n",
      "step: 16, train loss: 0.9942324, train accuracy: 0.5\n",
      "step: 16, test loss: 1.0188067, test accuracy: 0.47260273\n",
      "step: 20, train loss: 0.9044162, train accuracy: 0.625\n",
      "step: 20, test loss: 0.8960825, test accuracy: 0.59931505\n",
      "step: 24, train loss: 1.0571532, train accuracy: 0.5\n",
      "step: 24, test loss: 1.0216777, test accuracy: 0.59246576\n",
      "step: 28, train loss: 0.82417476, train accuracy: 0.59375\n",
      "step: 28, test loss: 0.80982697, test accuracy: 0.6438356\n",
      "step: 32, train loss: 0.9238791, train accuracy: 0.53125\n",
      "step: 32, test loss: 0.9979492, test accuracy: 0.55479455\n",
      "step: 36, train loss: 0.8902892, train accuracy: 0.53125\n",
      "step: 36, test loss: 0.927876, test accuracy: 0.5890411\n",
      "step: 40, train loss: 0.8269974, train accuracy: 0.75\n",
      "step: 40, test loss: 0.8532754, test accuracy: 0.5890411\n",
      "step: 44, train loss: 0.7766646, train accuracy: 0.65625\n",
      "step: 44, test loss: 0.78476995, test accuracy: 0.65753424\n",
      "step: 48, train loss: 0.86930764, train accuracy: 0.5625\n",
      "step: 48, test loss: 0.7988989, test accuracy: 0.6438356\n",
      "step: 52, train loss: 0.63427657, train accuracy: 0.71875\n",
      "step: 52, test loss: 0.8290549, test accuracy: 0.63013697\n",
      "step: 56, train loss: 0.6940655, train accuracy: 0.71875\n",
      "step: 56, test loss: 0.78640455, test accuracy: 0.65068495\n",
      "step: 60, train loss: 0.5418473, train accuracy: 0.875\n",
      "step: 60, test loss: 0.80593514, test accuracy: 0.6438356\n",
      "step: 64, train loss: 0.6875263, train accuracy: 0.65625\n",
      "step: 64, test loss: 0.70070016, test accuracy: 0.7226027\n",
      "step: 68, train loss: 0.65396714, train accuracy: 0.71875\n",
      "step: 68, test loss: 0.7546087, test accuracy: 0.6815069\n",
      "step: 72, train loss: 0.5196738, train accuracy: 0.8125\n",
      "step: 72, test loss: 0.74019724, test accuracy: 0.6849315\n",
      "step: 76, train loss: 0.60814273, train accuracy: 0.75\n",
      "step: 76, test loss: 0.8296125, test accuracy: 0.67808217\n",
      "step: 80, train loss: 0.4723729, train accuracy: 0.8125\n",
      "step: 80, test loss: 0.71943575, test accuracy: 0.72602737\n",
      "step: 84, train loss: 0.5504999, train accuracy: 0.8125\n",
      "step: 84, test loss: 0.8566901, test accuracy: 0.66780823\n",
      "step: 88, train loss: 0.45930946, train accuracy: 0.875\n",
      "step: 88, test loss: 0.873899, test accuracy: 0.6643836\n",
      "step: 92, train loss: 0.52173185, train accuracy: 0.78125\n",
      "step: 92, test loss: 0.78885067, test accuracy: 0.6917808\n",
      "step: 96, train loss: 0.49905497, train accuracy: 0.8125\n",
      "step: 96, test loss: 0.97165346, test accuracy: 0.57191783\n"
     ]
    }
   ],
   "source": [
    "# batch training\n",
    "test_X, test_y, test_seqlen, test_aspects = u.getData('test')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        batch_X, batch_y, batch_seqlen, batch_aspects = u.nextBatch(batch_size)\n",
    "        sess.run(optimizer, feed_dict = {X: batch_X, y: batch_y, seqlen: batch_seqlen, aspects: batch_aspects})\n",
    "        if i > 0 and i % 4 == 0:\n",
    "            loss_train, accuracy_train = sess.run([loss, accuracy], feed_dict = {X: batch_X, y: batch_y, seqlen: batch_seqlen, aspects: batch_aspects})\n",
    "            print('step: %s, train loss: %s, train accuracy: %s' % (i, loss_train, accuracy_train))\n",
    "            loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, aspects: test_aspects})\n",
    "            print('step: %s, test loss: %s, test accuracy: %s' % (i, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
